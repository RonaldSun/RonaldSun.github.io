---

title: 3D检测
date: 2025-05-05 22:58:55 +0800
categories: [论文, 3D, detection]
tags: [论文, 3D, detection]
pin: true
---

## Sparse4D(2023 horizon)

BEV based检测方法缺点：

1. 在感知范围、感知精度、计算效率这几个方面总要做一定的取舍；
2. 在高程上总有一些信息损失；

这片文章用DETR的方式，直接用3D空间中的query提取2D空间的feature：

1. 在box中取了一些keypoint点，在multi scale、 multi view上提取特征，然后在时序上提取特征：
   - 每个instance取7个固定的reference points，和其他K个通过模型算出来的reference point一起作为keypoints set;
   - 每个keypoint根据预测的instance速度以及ego的pose，计算其在上一帧的位置坐标
   - 首先是单帧的特征提取：把这些keypoints投影到multi-view和multi-scale的图像上用双线性插值取特征，然后使用预测的weight加权求和；这里预测的weight会分为多个组，类似于transformer中multihead的做法；
   - 然后是时序上的特征融合，从最后一个时刻开始，两两时刻的特征cat之后过一个linear层，直到最后一层
   - 最后instance的特征是所有keypoint的特征之和
2. depth reweight：为了解决远处的box投影到图片上会取到近处物体的feature, 所以有了一个加权的策略，如果预测的depth不准，那么大概率就是没取到正确的feature；
3. init reference bbox使用gt做kmeans得到的。

比较值得注意的是这篇文章的特征提取方式，可以看成是一种在3D空间中的Deformable attention的实现方式，并没有像BEVFormer中那样去2D里面做deformable。
匀速运动假设、heading不变以及速度估计的精度可能是隐含的问题。

## Sparse4D v2(2023 horizon)

1. 去掉了历史的图像feature，只保留上一帧的instance feature；
2. 先单帧预测k个instacne，然后把上一帧的n个预测结果wrap到当前帧，和并在一起得到k+n个query;
3. 接着在当前帧做类似的decoder，输出最终的instance；
4. 在预测weight的时候加入了相机内外参的encoding feature，而不是按照固定的layout方式预测
5. 用dense depth预测替换了depth reweight模块

## Sparse4D v3(2023 horizon)

1. 加入了denoising的训练策略，对GT加两种噪声，另外还会把上一帧的预测结果也加一些噪声wrap到当前帧作为辅助训练的query；
2. 增加了对中心点和heading角的置信度预测；
3. 采用了decouple attention的结构，把content feature和anchor feature concat之后做attention
4. 添加了tracking的逻辑，每一帧的instance会分配ID，来自上一帧的query的confidence如果大于阈值，就会直接和GT匹配
